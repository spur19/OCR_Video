{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python390jvsc74a57bd016e6d4416837aa69ac450a97991d2e9bdda02c348f23ce30b96dd812591fc704",
   "display_name": "Python 3.9.0 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "16e6d4416837aa69ac450a97991d2e9bdda02c348f23ce30b96dd812591fc704"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the arguments for the model\n",
    "# video : Path to video file\n",
    "# east : Path to EAST scene text detector model file\n",
    "# min_confidence : Probability threshold to determine text\n",
    "# width :Resized frame width (must be a multiple of 32)\n",
    "# height : Resized frame height (must be a multiple of 32)\n",
    "# padding : amount of padding to add to each border of ROI\n",
    "args = {'video':\"source_hq.mp4\", 'min_confidence' : 0.6 , 'east': 'frozen_east_text_detection.pb', 'width': 1280 , 'height' : 704, 'padding' : 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import imutils\n",
    "from imutils.object_detection import non_max_suppression\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "import argparse\n",
    "import cv2\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Mention the installed location of Tesseract-OCR in the system\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  \n",
    "\n",
    "def decode_predictions(scores, geometry):\n",
    "\t# grab the number of rows and columns from the scores volume, then\n",
    "\t# initialize our set of bounding box rectangles and corresponding\n",
    "\t# confidence scores\n",
    "\t(numRows, numCols) = scores.shape[2:4]\n",
    "\trects = []\n",
    "\tconfidences = []\n",
    "\t# loop over the number of rows\n",
    "\tfor y in range(0, numRows):\n",
    "\t\t# extract the scores (probabilities), followed by the\n",
    "\t\t# geometrical data used to derive potential bounding box\n",
    "\t\t# coordinates that surround text\n",
    "\t\tscoresData = scores[0, 0, y]\n",
    "\t\txData0 = geometry[0, 0, y]\n",
    "\t\txData1 = geometry[0, 1, y]\n",
    "\t\txData2 = geometry[0, 2, y]\n",
    "\t\txData3 = geometry[0, 3, y]\n",
    "\t\tanglesData = geometry[0, 4, y]\n",
    "\t\t# loop over the number of columns\n",
    "\t\tfor x in range(0, numCols):\n",
    "\t\t\t# if our score does not have sufficient probability,\n",
    "\t\t\t# ignore it\n",
    "\t\t\tif scoresData[x] < args[\"min_confidence\"]:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# compute the offset factor as our resulting feature\n",
    "\t\t\t# maps will be 4x smaller than the input image\n",
    "\t\t\t(offsetX, offsetY) = (x * 4.0, y * 4.0)\n",
    "\t\t\t# extract the rotation angle for the prediction and\n",
    "\t\t\t# then compute the sin and cosine\n",
    "\t\t\tangle = anglesData[x]\n",
    "\t\t\tcos = np.cos(angle)\n",
    "\t\t\tsin = np.sin(angle)\n",
    "\t\t\t# use the geometry volume to derive the width and height\n",
    "\t\t\t# of the bounding box\n",
    "\t\t\th = xData0[x] + xData2[x]\n",
    "\t\t\tw = xData1[x] + xData3[x]\n",
    "\t\t\t# compute both the starting and ending (x, y)-coordinates\n",
    "\t\t\t# for the text prediction bounding box\n",
    "\t\t\tendX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
    "\t\t\tendY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
    "\t\t\tstartX = int(endX - w)\n",
    "\t\t\tstartY = int(endY - h)\n",
    "\t\t\t# add the bounding box coordinates and probability score\n",
    "\t\t\t# to our respective lists\n",
    "\t\t\trects.append((startX, startY, endX, endY))\n",
    "\t\t\tconfidences.append(scoresData[x])\n",
    "\t# return a tuple of the bounding boxes and associated confidences\n",
    "\treturn (rects, confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] loading EAST text detector...\n"
     ]
    }
   ],
   "source": [
    "# initialize the original frame dimensions, new frame dimensions,\n",
    "# and ratio between the dimensions\n",
    "(W, H) = (1280, 720)\n",
    "(newW, newH) = (args[\"width\"], args[\"height\"])\n",
    "(rW, rH) = (W/float(newW),H/float(newH))\n",
    "# define the two output layer names for the EAST detector model that\n",
    "# we are interested -- the first is the output probabilities and the\n",
    "# second can be used to derive the bounding box coordinates of text\n",
    "layerNames = [\n",
    "\t\"feature_fusion/Conv_7/Sigmoid\",\n",
    "\t\"feature_fusion/concat_3\"]\n",
    "# load the pre-trained EAST text detector\n",
    "print(\"[INFO] loading EAST text detector...\")\n",
    "net = cv2.dnn.readNet(args[\"east\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Video Capture\n",
    "vs = cv2.VideoCapture(\"source_hq.mp4\")\n",
    "# set FPS to 10\n",
    "vs.set(cv2.CAP_PROP_FPS, 10)\n",
    "# start the FPS throughput estimator\n",
    "fps = FPS().start()\n",
    "# initialize the list of results\n",
    "res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-164272c6ab05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblobFromImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"width\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"height\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m123.68\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m116.78\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m103.94\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswapRB\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgeometry\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayerNames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;31m# decode the predictions, then  apply non-maxima suppression to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# suppress weak, overlapping bounding boxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loop over frames from the video stream\n",
    "while True:\n",
    "\t# grab the current frame of the VideoCapture object\n",
    "\tframe = vs.read()\n",
    "\tframe = frame[1] if args.get(\"video\", False) else frame\n",
    "\t# check to see if we have reached the end of the stream\n",
    "\tif frame is None:\n",
    "\t\tbreak\n",
    "\torig = frame.copy()\n",
    "\t(origH, origW) = frame.shape[:2]\n",
    "\t# resize the frame\n",
    "\tframe = cv2.resize(frame, (args[\"width\"], args[\"height\"]))\n",
    "\t# construct a blob from the image and then perform a forward pass of\n",
    "\t# the model to obtain the two output layer sets\n",
    "\tblob = cv2.dnn.blobFromImage(frame, 1.0, (args[\"width\"], args[\"height\"]),(123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "\tnet.setInput(blob)\n",
    "\t(scores, geometry) = net.forward(layerNames)\n",
    "\t# decode the predictions, then  apply non-maxima suppression to\n",
    "\t# suppress weak, overlapping bounding boxes\n",
    "\t(rects, confidences) = decode_predictions(scores, geometry)\n",
    "\tboxes = non_max_suppression(np.array(rects), probs=confidences)\n",
    "\t\n",
    "\t# loop over the bounding boxes\n",
    "\tfor (startX, startY, endX, endY) in boxes:\n",
    "\t\t# scale the bounding box coordinates based on the respective\n",
    "\t\t# ratios\n",
    "\t\tstartX = int(startX * rW)\n",
    "\t\tstartY = int(startY * rH)\n",
    "\t\tendX = int(endX * rW)\n",
    "\t\tendY = int(endY * rH)\n",
    "\t\t# in order to obtain a better OCR of the text we can potentially\n",
    "\t\t# apply a bit of padding surrounding the bounding box -- here we\n",
    "\t\t# are computing the deltas in both the x and y directions\n",
    "\t\tdX = int((endX - startX) * args[\"padding\"])\n",
    "\t\tdY = int((endY - startY) * args[\"padding\"])\n",
    "\t\t# apply padding to each side of the bounding box, respectively\n",
    "\t\tstartX = max(0, startX - dX)\n",
    "\t\tstartY = max(0, startY - dY)\n",
    "\t\tendX = min(origW, endX + (dX * 2))\n",
    "\t\tendY = min(origH, endY + (dY * 2))\n",
    "\t\t# extract the actual padded ROI\n",
    "\t\troi = orig[startY:endY, startX:endX]\n",
    "\n",
    "\t\t# in order to apply Tesseract v4 to OCR text we must supply\n",
    "\t\t# (1) a language, (2) an OEM flag of 4, indicating that the we\n",
    "\t\t# wish to use the LSTM neural net model for OCR, and finally\n",
    "\t\t# (3) an OEM value, in this case, 7 which implies that we are\n",
    "\t\t# treating the ROI as a single line of text\n",
    "\t\tconfig = (\"-l eng --oem 1 --psm 7\")\n",
    "\t\ttext = pytesseract.image_to_string(roi, config=config)\n",
    "\t\t# draw the bounding box on the frame\n",
    "\t\tcv2.rectangle(orig, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "\t\t# add the bounding box coordinates and OCR'd text to the list\n",
    "\t\t# of results\n",
    "\t\tres.append(((startX, startY, endX, endY), text))\n",
    "\t# update the FPS counter\n",
    "\tfps.update()\n",
    "\t# show the output image\n",
    "\tcv2.imshow(\"Text Detection\", orig)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    "\t# if the `q` key was pressed, break from the loop\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "# stop the timer and display FPS information\n",
    "fps.stop()\n",
    "print(\"[INFO] elasped time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "# Release the pointer \n",
    "vs.release()\n",
    "# close all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OCR TEXT\n========\nWe\n\nEvent?\n\nwe\n\nFV\n\nPAV\n\nWil\n\nbuals\n\nVey\n\nthe\n\nWs\n\nMissed\n\n"
     ]
    }
   ],
   "source": [
    "# sort the results bounding box coordinates from top to bottom\n",
    "res = sorted(res, key=lambda r:r[0][1])\n",
    "\n",
    "# clean the results obtained by Tesseract OCR\n",
    "filtered = set()\n",
    "# Remove duplicate words, words with unexpected symbols\n",
    "for ((startX, startY, endX, endY), text) in res:\n",
    "\ttokens = text.strip().split()\n",
    "\tclean_tokens =  [t for t in tokens if re.match(r'[^''-$%^&*()«_®+|~=.{}<>\\[\\]:\";`\\/]*$', t)]\n",
    "\tclean_s = ' '.join(clean_tokens)\n",
    "\tif(len(clean_s)>1 and not(bool(re.search(r\"\\s\", clean_s)))):\n",
    "\t\tfiltered.add(clean_s)\n",
    "\n",
    "# Write results obtained to a text file\n",
    "file1 = open('results.txt', 'w')\n",
    "\n",
    "\n",
    "# display the text OCR'd by Tesseract\n",
    "print(\"OCR TEXT\")\n",
    "print(\"========\")\t\n",
    "for word in filtered:\t\n",
    "\tprint(\"{}\\n\".format(word))\n",
    "\tfile1.write(word)\n",
    "\tfile1.write(\"\\n\")\n",
    "\n",
    "# Close the file\n",
    "file1.close()\t"
   ]
  }
 ]
}